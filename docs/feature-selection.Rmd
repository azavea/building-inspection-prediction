---
title: "Feature Selection"
---
We have created a large dataset with 89 attributes. Not all of these will be useful predictors. In this section, we will narrow down our dataset by selecting only the features that will be useful to predict for the inspection outcome. Some of the model types that we'll experiment have built-in feature selection while others don't. Either way, it will lead to better results if we remove variables that are redundant or simply just random noise. There are several methods for variable selection but in this case we used a feature selection algorithm called "Boruta" from an R package with the same name.

## Boruta
The Boruta algorithm relies on variable importance values that it repeatedly calculates by running Random Forest models on your dataset for a predefined set of predictor variables. It creates a set of random noise variables associated with each attribute by shuffling each value's row location. After training a Random Forest model it checks the variable importance of each feature against the distribution of corresponding noise variables. With t-tests it determines whether the real predictor is significantly more important than the noise variables. If so it is confirmed as important. The algorithm runs until it has separated all of the variables or it hits a user-specified limit on the number of iterations to go through. This process is expensive and may need hours to run but it neatly separates your variables into useful subsets.

```{r eval=FALSE}

# packages
packages(c("tidyverse", "caret", "Boruta"))

# extract just the predictor variable candidates
dd <- ds %>%
  select(one_of(mod_vars))

# partition dataset for boruta
set.seed(12345)
inTrain <- createDataPartition(dd$o.failed.n, p = 0.6, list = FALSE)
boruta.train.set <- dd[inTrain, -c(1,2)]

# run iterative random forest variable importance test
boruta.train <- Boruta(o.failed ~ ., data = train, doTrace = 2)
# get final decision
fd <- boruta.train$finalDecision
rejected_vars <- fd[which(fd != "Confirmed")] %>% names
```

The algorithm determines which variables normalize to have no impact. We can look at these rejected variables:

```{r eval=TRUE, echo=FALSE}
load("../data/boruta.train.Rdata")
```
```{r eval=TRUE}
# generate a list of non-useful variables
# get final decision
print(rejected_vars)
```

and then the variables that were accepted:

```{r eval=TRUE, echo=FALSE}
accepted_vars <- setdiff(names(boruta.train.set), rejected_vars)
print(accepted_vars)
```

Now we have a list of variables that we want to include as predictors when building the model.

## Variable Importance
We are jumping ahead a little bit here, but let's look at the variable importance from the Random Forest and Gradient Boosting Machine models we will create in the next section.

![](plots/fig.x-Variable_importance_plot.svg)

Interestingly, the importance of some variables varied slightly between the RF and GBM models. However, for the most part, the most powerful predictors carried over from model to model. The categorical inspection description variables, as well our time series predictors proved to be most important.

I the [next section](model-selection.html) we experiment with different machine learning algorithms and select a final model.
